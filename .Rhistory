length(grep(uniques[i], unlist(strsplit(examp1, split=" "))))
B <- length(grep(uniques[i], unlist(strsplit(examp1, split=" "))))
log ( (A * B * sizeCorpus) / (A * B * span) ) / log (2)
(A * B * sizeCorpus)
(A * B * span)
(A * B * sizeCorpus) / (A * B * span)
i <- 5
B <- length(grep(uniques[i], unlist(strsplit(examp1, split=" "))))
B
( (A * B * sizeCorpus) / (A * B * span) )
B
sizeCorpus
span
View(LHS_means)
View(LHS)
allwords
LHS_freqs <- sapply(uniques, function(i) length(grep(i, allwords))
)
LHS_freqs
allwords
grep(i, allwords)
grep(unique[1], allwords)
grep(unique[1], allwords)
unique[1]
grep(unique[[1]], allwords)
unique[[1]]
unique
grep(uniques[[1]], allwords)
grep(uniques[[2]], allwords)
grep(uniques[[3]], allwords)
grep(uniques[1], allwords)
grep(uniques[2], allwords)
grep(uniques[3], allwords)
allwords
LHS_freqs <- sapply(uniques, function(i) length(grep(i, unlist(strsplit(allwords, split = " ")))))
LHS_freqs
LHS_means
LHS_means <- rowMeans(LHS, na.rm = TRUE)
LHS_means <- data.frame(word = names(LHS_means),
mean_dist = unname(LHS_means))
# also get coloc frequencies in spans
LHS_freqs <- sapply(uniques, function(i) length(grep(i, unlist(strsplit(allwords, split = " ")))))
LHS_means$freqs <- LHS_freqs
LHS_means
LHS_means <- LHS_means[with(LHS_means, order(mean_dist)), ]
LHS_means
# sort by frequency
LHS_means <- LHS_means[with(LHS_means, order(freqs)), ]
LHS_means
LHS_means <- LHS_means[with(LHS_means, -order(freqs)), ]
LHS_means
row.names(LHS) <- uniques
# compute mean distance between the two words
LHS_means <- rowMeans(LHS, na.rm = TRUE)
LHS_means <- data.frame(word = names(LHS_means),
mean_dist = unname(LHS_means))
# also get coloc frequencies in spans
LHS_freqs <- sapply(uniques, function(i) length(grep(i, unlist(strsplit(allwords, split = " ")))))
LHS_means$freqs <- LHS_freqs
# sort by mean distance
LHS_means <- LHS_means[with(LHS_means, order(mean_dist)), ]
# sort by frequency
LHS_means <- LHS_means[with(LHS_means, order(-freqs)), ]
LHS_means
LHS_freqs <- sapply(uniques, function(i) length(grep(i, unlist(strsplit(subset_ngrams, split = " ")))))
LHS_freqs
subset_ngrams
unlist(strsplit(subset_ngrams, split = " "))
subset_ngrams
LHS_means$freqs <- LHS_freqs
# sort by mean distance
LHS_means <- LHS_means[with(LHS_means, order(mean_dist)), ]
# sort by frequency
LHS_means <- LHS_means[with(LHS_means, order(-freqs)), ]
LHS_means
uniques
length(grep('a', unlist(strsplit(subset_ngrams, split = " "))))
subset_ngrams
unlist(strsplit(subset_ngrams, split = " "))
sum(unlist(strsplit(subset_ngrams, split = " "))=="a")
LHS_freqs <- sapply(uniques, function(i) sum(unlist(strsplit(subset_ngrams, split = " "))==i))
LHS_freqs
LHS_means$freqs <- LHS_freqs
# sort by mean distance
LHS_means <- LHS_means[with(LHS_means, order(mean_dist)), ]
# sort by frequency
LHS_means <- LHS_means[with(LHS_means, order(-freqs)), ]
LHS_means
LHS_means <- LHS_means[with(LHS_means, order(mean_dist)), ]
LHS_means
subset_ngrams
LHS_freqs <- sapply(uniques, function(i) sum(unlist(strsplit(subset_ngrams, split = " "))==i))
LHS_freqs
LHS_freqs <- sapply(uniques, function(i) sum(unlist(strsplit(allwords, split = " "))==i))
LHS_freqs
rowMeans(LHS, na.rm = TRUE)
LHS_means <- rowMeans(LHS, na.rm = TRUE)
LHS_means <- data.frame(word = names(LHS_means),
mean_dist = unname(LHS_means))
# also get coloc frequencies in spans
LHS_freqs <- sapply(uniques, function(i) sum(unlist(strsplit(allwords, split = " "))==i))
LHS_means$freqs <- LHS_freq
LHS_means$freqs <- LHS_freqs
LHS_means
LHS_means <- LHS_means[with(LHS_means, order(-freqs)), ]
LHS_means
i
pos1 <- sapply(uniques, function(x) which(x == unlist(strsplit(subset_ngrams[[i]], split=" "))))
# find position of word of interest along ngram vector
pos2 <- which(word == unlist(strsplit(subset_ngrams[[i]], split=" ")) )
# compute distance of all colocs to word of interest
dist <- lapply(pos1, function(i) pos2 - i )
pos2
dist
dist
LHS
sum( !is.na( LHS ) )
apply(LHS, 1, sum( !is.na( LHS ) )
)
?apply
countN <- function ( v ) {
sum ( !is.na ( v ) ) - sum ( is.na ( v ) )
}
LHS_freqs <- apply(LHS, 1, countN )
LHS_freqs
countN <- function ( v ) {
sum( !is.na( v ) )
}
countN <- function ( v ) sum( !is.na( v ) )
LHS_freqs <- apply(LHS, 1, countN )
LHS_freqs
LHS_means <- rowMeans(LHS, na.rm = TRUE)
LHS_means <- data.frame(word = names(LHS_means),
mean_dist = unname(LHS_means),
freq = unname(LHS_freqs))
LHS_means <- LHS_means[with(LHS_means, order(mean_dist)), ]
LHS_means
LHS_means <- LHS_means[with(LHS_means, order(-freqs)), ]
LHS_means <- LHS_means[with(LHS_means, order(-freq)), ]
LHS_means
subset_ngrams
MI <- vector(length = length(uniques))
for(i in 1:length(uniques)){
# A = frequency of node word
A <- length(grep(word, unlist(strsplit(examp1, split=" "))))
# B = frequency of collocate
B <- length(grep(uniques[i], unlist(strsplit(examp1, split=" "))))
# size of corpus = number of words in total
sizeCorpus <- length(unlist(strsplit(examp1, split=" ")))
# span = span of words analysed to L and R of node word
span <- span
# compute MI
MI[i] <- log ( (A * B * sizeCorpus) / (A * B * span) ) / log (2)
}
MI
RHS <- data.frame(matrix(nrow = length(uniques), ncol = length(subset_ngrams)))
for(i in 1:length(subset_ngrams)){
# find position of unique words along ngram vector
pos1 <- sapply(uniques, function(x) which(x == unlist(strsplit(subset_ngrams[[i]], split=" "))))
# find position of word of interest along ngram vector
pos2 <- which(word == unlist(strsplit(subset_ngrams[[i]], split=" ")) )
# compute distance of all colocs to word of interest
dist <- lapply(pos1, function(i) pos2 - i )
# keep only +ve values
dist <- lapply(dist, function(i)  i[i<0][1]  )
# insert distance values into a vector to
# append into a data frame
tmp <- rep(NA, length(uniques))
tmp <- tmp[1:length(unlist(unname(dist)))] <- unlist(unname(dist))
RHS[,i] <- tmp
}
RHS
row.names(LHS) <- uniques
row.names(RHS) <- uniques
}
row.names(RHS) <- uniques
RHS
# compute mean distance between the two words
RHS_means <- rowMeans(RHS, na.rm = TRUE)
RHS_means
# function to count non-NA values
countN <- function ( v ) sum( !is.na( v ) )
RHS_freqs <- apply(RHS, 1, countN )
RHS_means <- data.frame(word = names(RHS_means),
mean_dist = unname(RHS_means),
freq = unname(LRHS_freqs))
RHS_means <- data.frame(word = names(RHS_means),
mean_dist = unname(RHS_means),
freq = unname(RHS_freqs))
RHS_means <- RHS_means[with(RHS_means, order(mean_dist)), ]
# sort by frequency
RHS_means <- RHS_means[with(RHS_means, order(-freq)), ]
RHS_means
RHS_means <- RHS_means[with(RHS_means, order(mean_dist)), ]
RHS_means
RHS_means <- RHS_means[with(RHS_means, order(-mean_dist)), ]
RHS_means
RHS_means <- RHS_means[with(RHS_means, order(-freq)), ]
RHS_means
install.packages("lle")
require(lle)
demo()
demo(lle)
data( lle_scurve_data )
View(lle_scurve_data)
X <- lle_scurve_data
results <- lle( X=X, m=2, k=12, reg=2, ss=FALSE, id=TRUE, v=0.9 )
str( results )
split.screen( c(2,1) )
screen(1)
plot( results$Y, main="embedded data", xlab=expression(y[1]), ylab=expression(y[2]) )
plot( results$Y, main="embedded data", xlab=expression(y[1]), ylab=expression(y[2]) )
plot( results$id, main="intrinsic dimension", type="l", xlab=expression(x[i]), ylab="id", lwd=2 )
GMH <- read.table(header= TRUE, text = "material depth age error
Shell    0.23	1010.5	47.5
Shell  	0.49	1464.5	58.5
Organics  	0.49	507.5	22.5
Shell  	0.5	1613.5	78.5
Shell  	1.03	4735	109
Organics  	1.26	3733.5	94.5
Shell  	1.45	5454.5	129.5
Organics  	1.45	4334	86
Shell  	1.45	5453	128
Organics  	1.45	4286	127
Shell  	1.51	5450	129
Shell  	1.68	7638	45
Shell  	1.86	7458.5	40.5
Organics  	1.86	6147.5	131.5
Organics  	2.14	6256	49
Organics  	2.19	6040.5	130.5
Shell  	2.22	7686.5	72.5
Organics  	2.22	6256	51
Shell  	2.45	7586.5	77.5
Organics  	2.45	6388.5	76.5
Shell  	2.6	8093.5	77.5
")
# basic linear regression
(summary(GMHlm <- with(GMH, lm(age ~ depth))))
# age = 2994.9 * depth + 420.1
# by material type
require(plyr)
ddply(GMH, "material", summarize, rsq = summary(lm(age ~ depth))$r.squared)
dlply(GMH, "material", function(df) summary(lm(age ~ depth, data = df)))
# check for differences in intercept to get offset
ddply(GMH, "material", function(df) coef(lm(age ~ depth, data = df))["(Intercept)"])
# plot
library(ggplot2)
ggplot(GMH, aes(age, depth)) + geom_point() # all
ggplot(GMH[GMH$material == 'Organics',], aes(age, depth)) + geom_point() # only organics
ggplot(GMH[GMH$material == 'Shell',], aes(age, depth)) + geom_point() # only shell
# Define the top and bottom of the errorbars
limits <- with(GHM, aes(ymax = age + error, ymin=age - error))
ggplot(GMH, aes( depth, age, colour = material))   + # all, by colour
geom_point(size = 3) +
geom_pointrange(limits, width = 2) +
coord_flip() +
xlab("meters below surface") +
ylab("cal yrs BP") +
theme(#legend.position = "none",
#panel.background = element_blank(),          # suppress default background
#panel.grid.major = element_blank(),              # suppress default major gridlines
#panel.grid.minor = element_blank(),              # suppress default minor gridlines
#axis.ticks = element_blank(),                    # suppress tick marks
axis.title.x=element_text(colour="black", size = 15),              # increase axis title size slightly
axis.title.y=element_text(colour="black", angle=90, size = 15),    # increase axis title size slightly and rotate
axis.text.x=element_text(colour="black", size = 15),               # increase size of numbers on x-axis
axis.text.y=element_text(colour="black", size = 15))               # increase size of numbers on y-axis
limits <- with(GHM, aes(ymax = age + error, ymin=age - error))
ggplot(GMH, aes( depth, age, colour = material))   + # all, by colour
geom_point(size = 3) +
geom_pointrange(limits, width = 2) +
coord_flip() +
xlab("meters below surface") +
ylab("cal yrs BP") +
theme(#legend.position = "none",
#panel.background = element_blank(),          # suppress default background
#panel.grid.major = element_blank(),              # suppress default major gridlines
#panel.grid.minor = element_blank(),              # suppress default minor gridlines
#axis.ticks = element_blank(),                    # suppress tick marks
axis.title.x=element_text(colour="black", size = 15),              # increase axis title size slightly
axis.title.y=element_text(colour="black", angle=90, size = 15),    # increase axis title size slightly and rotate
axis.text.x=element_text(colour="black", size = 15),               # increase size of numbers on x-axis
axis.text.y=element_text(colour="black", size = 15))               # increase size of numbers on y-axis
limits <- with(GHM, aes(ymax = age + error, ymin=age - error))
limits <- with(GMH, aes(ymax = age + error, ymin=age - error))
ggplot(GMH, aes( depth, age, colour = material))   + # all, by colour
geom_point(size = 3) +
geom_pointrange(limits, width = 2) +
coord_flip() +
xlab("meters below surface") +
ylab("cal yrs BP") +
theme(#legend.position = "none",
#panel.background = element_blank(),          # suppress default background
#panel.grid.major = element_blank(),              # suppress default major gridlines
#panel.grid.minor = element_blank(),              # suppress default minor gridlines
#axis.ticks = element_blank(),                    # suppress tick marks
axis.title.x=element_text(colour="black", size = 15),              # increase axis title size slightly
axis.title.y=element_text(colour="black", angle=90, size = 15),    # increase axis title size slightly and rotate
axis.text.x=element_text(colour="black", size = 15),               # increase size of numbers on x-axis
axis.text.y=element_text(colour="black", size = 15))               # increase size of numbers on y-axis
2994.9 * 0.9 + 420.1
dlply(GMH, "material", function(df) summary(lm(age ~ depth, data = df)))
3030.4 * 0.9 -339.1
3030.4 * 0.8 -339.1
library(tm)
data(crude)
crude[[1]]
cat(paste0(1,/t, crude[[1]]))
paste0(1,/t, crude[[1]]))
paste0(1,/t, crude[[1]])
paste0(1,\t, crude[[1]])
paste0(1, "\t", crude[[1]])
length(crude)
for(i in 1:length(crude)){
cat(paste0(i, "\t", crude[[i]]))
}
sink()
sink(example.txt)
for(i in 1:length(crude)){
cat(paste0(i, "\t", crude[[i]]))
}
sink()
sink("example.txt")
for(i in 1:length(crude)){
cat(paste0(i, "\t", crude[[i]]))
}
sink()
sink("example.txt")
for(i in 1:length(crude)){
cat(paste0(i, "\t", grep("\n", " ", crude[[i]])))
}
sink()
grep("\n", " ", crude[[i]])
?grep
sink("example.txt")
for(i in 1:length(crude)){
cat(paste0(i, "\t", gsub("\n", " ", crude[[i]])))
}
sink()
gsub("\n", " ", crude[[i]])
sink("example.txt")
for(i in 1:length(crude)){
cat(paste0(i, "\t", gsub("\n", " ", crude[[i]])))
cat("\n")
}
sink()
sink("example.txt")
for(i in 1:length(crude)){
cat(paste0(i, "\t", gsub("\n", " ", crude[[i]])))
cat("\n")
}
sink()
> reuters <- tm_map(reuters, as.PlainTextDocument)
reuters <- tm_map(reuters, as.PlainTextDocument)
reut21578
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- Corpus(DirSource(reut21578),
+ readerControl = list(reader = readReut21578XML))
reut21578
reuters <- Corpus(DirSource(reut21578),
+ readerControl = list(reader = readReut21578XML))
reuters <- Corpus(DirSource(reut21578), readerControl = list(reader = readReut21578XML))
reuters
fix(reuters)
### load data
# where is the data file?
wd <- "C:/Users/marwick/Documents/GitHub/reanalysis-of-marwick-2008-JAS"
setwd(wd)
dat <- read.csv("Marwick_2008_JAS_data.csv")
```
```{r, echo=FALSE, message=FALSE}
### prepare data
# get flake sequence numbers
dat$flake_seq <- as.numeric(gsub("\\D", "", dat$HAPP.artefact.number))
# get core IDs
dat$core_id <- gsub("[[:digit:]]", "", dat$HAPP.artefact.number)
# recode core_ids
# just keep the first two characters of core id
# other characters denote ties in flaking sequence
dat$core_id <- substring(dat$core_id, 1, 2)
# if the first character is not 'A' then drop second character
dat$core_id <- ifelse(substring(dat$core_id, 1, 1) != 'A', substring(dat$core_id, 1, 1), dat$core_id)
# subset to use only complete flakes
dat <- dat[dat$Flake.Type =='Complete', ]
```
First, overhang removal.
```{r,  echo=TRUE}
### analyse data
## overhang removal
# recode OHR as 0 or 1
unique(dat$Overhang.removal)
dat$Overhang.removal <- with(dat, ifelse(Overhang.removal == 'Present', 1, 0 ))
# use logistic regression to investigate
# relationship between presence of OHR
# and position of flake in removal sequence
library(plyr)
# Break up dat by core_id, then fit the
# specified model to each piece and
# return a list
models <- dlply(dat, "core_id ", function(df)
glm(Overhang.removal ~ flake_seq, family=binomial(logit), data=df)   )
# Print the summary of each model
l_ply(models, summary, .print = TRUE)
# plot all models
lapply(models, function(i) plot(i$fitted) )
# Apply p-values to each model and return a data frame
tmp <- ldply(models, function(i) summary(i)$coef[, "Pr(>|z|)"][2])
# subset where p < 0.05
tmp1 <- tmp[ tmp$flake_seq < 0.5, ]
# none! so subset where p < 0.1
tmp2 <- tmp[ tmp$flake_seq < 0.5, ]
models_p <- models[na.omit(tmp2$core_id)]
# plot models where p < 0.1
lapply(models_p, function(i) plot(i$fitted) )
par(mfrow=c(length(models_p),2))
lapply(models_p, function(i) plot(i$fitted) )
par(mfrow=c(length(models_p),2))
lapply(models_p, function(i) plot(i$fitted) )
par(mfrow=c(2, length(models_p)))
lapply(models_p, function(i) plot(i$fitted) )
par(mfrow=c(2, length(models_p)/2))
lapply(models_p, function(i) plot(i$fitted) )
par(mfrow=c(2, length(models_p)/2))
lapply(models_p, function(i) plot(i$fitted) )
?par
par(mfrow=c(2, length(models_p)/2))
lapply(models_p, function(i) plot(i$fitted) )
par(mfrow=c(3, length(models_p)/2))
lapply(models_p, function(i) plot(i$fitted) )
par(mfrow=c(2, length(models_p)/2))
lapply(models_p, function(i) plot(i$fitted) )
install.packages("compactr")
library("compactr")
par(mfrow=c(2, length(models_p)/2))
lapply(models_p, function(i) eplot(i$fitted) )
library("ggplot2")
require("ggplot2")
require(reshape2)
h <- do.call(cbind, models_p)
h.melt <- melt(h)
View(h.melt)
h <- do.call(cbind, models_p$fitted)
models_p$fitted
models_p
View(h.melt)
lapply(models_p, function(i) (i$fitted) )
fit <- lapply(models_p, function(i) (i$fitted) )
h <- do.call(cbind, fit)
h.melt <- melt(h)
gplot(h.melt,aes(x=value,fill=N))+geom_dotplot()+facet_grid(N~.)+geom_rug()
ggplot(h.melt,aes(x=value,fill=N))+geom_dotplot()+facet_grid(N~.)+geom_rug()
View(h.melt)
ggplot(h.melt,aes(x=value,fill=Var1))+geom_dotplot()+facet_grid(Var1~.)+geom_rug()
ggplot(h.melt,aes(x=value,fill=Var1))+geom_dotplot()+facet_grid(Var2~.)+geom_rug()
ggplot(h.melt,aes(x=value,y=Var1))+geom_dotplot()+facet_grid(Var2~.)+geom_rug()
ggplot(h.melt,aes(x=value,y=Var1))+geom_point()+facet_grid(Var2~.)+geom_rug()
ggplot(h.melt,aes(x=Var1,y=value))+geom_point()+facet_grid(Var2~.)+geom_rug()
ggplot(h.melt,aes(x=Var1,y=value))+geom_point()+facet_grid(Var2~.)
ggplot(h.melt,aes(x=Var1,y=value))+geom_point()+facet_grid(Var2~., scales="free_y")
ggplot(h.melt,aes(x=Var1,y=value))+geom_point()+facet_grid(Var2~., scales="free_x")
ggplot(h.melt,aes(x=Var1,y=value))+geom_point()+facet_grid(Var2~., scales="free")
par(mfrow=c(2, length(models_p)/2))
fit
my.df <- do.call("rbind", lapply(fit, data.frame))
h
data.frame(lapply(data.frame(t(sapply(fit, `[`))), unlist))
fit
data.frame(t(sapply(fit, `[`)))
df  <- data.frame(t(sapply(fit, `[`)))
View(df)
df  <- data.frame(lapply(data.frame(t(sapply(fit, `[`))), unlist))
str9fit
str(fit)
h <- do.call(cbind, fit)
fit
h.melt <- melt(fit)
melt
h.melt
ggplot(h.melt,aes(x=L1,y=value))+geom_point()+facet_grid(L1~., scales="free")
fit <- lapply(models_p, function(i) data.frame(i$fitted) )
fit
h <- do.call(cbind, fit)
h.melt <- melt(fit)
melt
h.melt
h.melt <- melt(fit, id = row.names)
h.melt <- melt(fit, id = row.names(fit))
h.melt
fit
fit
h.melt <- melt(fit)
View(h.melt)
h.melt$seq <- unlist(sapply(rle(fit)$lengths, seq))
h.melt$seq <- unlist(sapply(rle(fit$L1)$lengths, seq))
fit$L1
h.melt$seq <- unlist(sapply(rle(h.melt$L1)$lengths, seq))
h.melt$seq
h.melt
ggplot(h.melt,aes(x=L1,y=seq))+geom_point()+facet_grid(L1~., scales="free")
View(h.melt)
ggplot(h.melt,aes(x=value,y=seq))+geom_point()+facet_grid(L1~., scales="free")
ggplot(h.melt,aes(x=value,y=seq))+geom_point()+facet_grid(L1~.)
ggplot(h.melt,aes(x=value,y=seq))+geom_point()+facet_grid(.~L1)
ggplot(h.melt,aes(x=value,y=seq))+geom_point()+facet_grid(L1~.)
ggplot(h.melt,aes(x=value,y=seq))+geom_point()+facet_grid(.~L1)
require(knitr)
require(markdown)
# process .md and .pdf files (including smart punctuation and grey background of code blocks)
filen <- "Marwick_2008_JAS_code" # name of this markdown file without suffix
knit(paste0(filen,".md"))
system(paste0("pandoc -s ", paste0(filen,"-out.md"), " -t latex -o ", paste0(filen,".pdf"), " --highlight-style=tango  -S"))
system(paste0("pandoc -s ", paste0(filen,"-out.md"), " -t latex -o ", paste0(filen,".pdf"), " --highlight-style=tango  -S"))
